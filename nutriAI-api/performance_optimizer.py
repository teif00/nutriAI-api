"""
Ù…Ø­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
Performance Optimizer for Enhanced Meal Planner
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from typing import Dict, List, Optional, Tuple
import time
import psutil
import gc
from contextlib import contextmanager
import logging

logger = logging.getLogger(__name__)

class PerformanceOptimizer:
    """Ù…Ø­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
    
    def __init__(self, model, device: str = "auto"):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡
        
        Args:
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ø³ÙŠÙ†Ù‡
            device: Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        """
        self.model = model
        self.device = self._setup_device(device)
        self.optimizer = None
        self.scheduler = None
        self.mixed_precision = False
        self.gradient_accumulation_steps = 1
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_stats = {
            "training_time": 0,
            "memory_usage": [],
            "gpu_utilization": [],
            "throughput": 0
        }
    
    def _setup_device(self, device: str) -> torch.device:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"""
        if device == "auto":
            if torch.cuda.is_available():
                return torch.device("cuda")
            elif torch.backends.mps.is_available():
                return torch.device("mps")
            else:
                return torch.device("cpu")
        return torch.device(device)
    
    def setup_mixed_precision(self, enabled: bool = True):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø®Ù„Ø· Ø§Ù„Ø¯Ù‚Ø©"""
        self.mixed_precision = enabled and self.device.type == "cuda"
        if self.mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()
            logger.info("âœ… ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø®Ù„Ø· Ø§Ù„Ø¯Ù‚Ø© (Mixed Precision)")
        else:
            logger.info("â„¹ï¸ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø®Ù„Ø· Ø§Ù„Ø¯Ù‚Ø© ØºÙŠØ± Ù…ØªØ§Ø­ Ø£Ùˆ Ù…Ø¹Ø·Ù„")
    
    def setup_gradient_accumulation(self, steps: int = 4):
        """Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª"""
        self.gradient_accumulation_steps = steps
        logger.info(f"âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ù„Ù€ {steps} Ø®Ø·ÙˆØ§Øª")
    
    def setup_optimizer(self, learning_rate: float = 3e-4, weight_decay: float = 0.01):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø­Ø³Ù† Ù…ØªÙ‚Ø¯Ù…"""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… AdamW Ù…Ø¹ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø­Ø³Ù†Ø©
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        logger.info(f"âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø­Ø³Ù† AdamW Ø¨Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… {learning_rate}")
    
    def setup_scheduler(self, num_training_steps: int, warmup_steps: int = 100):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
        from transformers import get_cosine_schedule_with_warmup
        
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=num_training_steps
        )
        
        logger.info(f"âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ø¹ {warmup_steps} Ø®Ø·ÙˆØ§Øª ØªØ³Ø®ÙŠÙ†")
    
    def setup_dataloader_optimization(self, dataloader: DataLoader):
        """ØªØ­Ø³ÙŠÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # ØªØ­Ø³ÙŠÙ† Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ø§Ù„
        if hasattr(dataloader, 'num_workers'):
            optimal_workers = min(psutil.cpu_count(), 8)
            dataloader.num_workers = optimal_workers
            logger.info(f"âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø¹Ø¯Ø¯ Ø¹Ù…Ø§Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ {optimal_workers}")
        
        # ØªØ­Ø³ÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
        if hasattr(dataloader, 'batch_size'):
            current_batch_size = dataloader.batch_size
            if self.device.type == "cuda":
                # Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ù„Ù„Ù€ GPU
                optimal_batch_size = min(current_batch_size * 2, 32)
                dataloader.batch_size = optimal_batch_size
                logger.info(f"âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø¥Ù„Ù‰ {optimal_batch_size}")
    
    @contextmanager
    def memory_monitoring(self):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            initial_cached = torch.cuda.memory_reserved()
            
            try:
                yield
            finally:
                final_memory = torch.cuda.memory_allocated()
                final_cached = torch.cuda.memory_reserved()
                
                memory_used = (final_memory - initial_memory) / 1024**3  # GB
                cached_used = (final_cached - initial_cached) / 1024**3  # GB
                
                self.performance_stats["memory_usage"].append({
                    "memory_used_gb": memory_used,
                    "cached_used_gb": cached_used,
                    "timestamp": time.time()
                })
                
                logger.info(f"ðŸ’¾ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory_used:.2f} GB (Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚Øª: {cached_used:.2f} GB)")
        else:
            yield
    
    def optimize_model_for_inference(self):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""
        logger.info("ðŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„...")
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        self.model.eval()
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if self.device.type == "cuda":
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ half precision
            self.model = self.model.half()
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            torch.cuda.empty_cache()
            
            # ØªÙØ¹ÙŠÙ„ cuDNN optimizations
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        # ØªØ¬Ù…ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        for param in self.model.parameters():
            param.requires_grad = False
        
        logger.info("âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„")
    
    def optimize_model_for_training(self):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        logger.info("ðŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.model.train()
        
        # ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
        for param in self.model.parameters():
            param.requires_grad = True
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        if self.device.type == "cuda":
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… float16 Ù„Ù„ØªØ¯Ø±ÙŠØ¨
            self.model = self.model.half()
            
            # ØªØ­Ø³ÙŠÙ† cuDNN
            torch.backends.cudnn.benchmark = True
        
        logger.info("âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
    
    def train_step_optimized(self, batch: Dict, step: int) -> Dict:
        """Ø®Ø·ÙˆØ© ØªØ¯Ø±ÙŠØ¨ Ù…Ø­Ø³Ù†Ø©"""
        start_time = time.time()
        
        with self.memory_monitoring():
            # Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¬Ù‡Ø§Ø²
            inputs = {k: v.to(self.device) for k, v in batch.items() if k != "labels"}
            labels = batch["labels"].to(self.device)
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø®Ù„Ø· Ø§Ù„Ø¯Ù‚Ø©
            if self.mixed_precision:
                with torch.cuda.amp.autocast():
                    outputs = self.model(**inputs, labels=labels)
                    loss = outputs.loss / self.gradient_accumulation_steps
                
                # ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
                self.scaler.scale(loss).backward()
                
                if (step + 1) % self.gradient_accumulation_steps == 0:
                    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
                    
                    # ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
                    if self.scheduler:
                        self.scheduler.step()
            else:
                outputs = self.model(**inputs, labels=labels)
                loss = outputs.loss / self.gradient_accumulation_steps
                
                # ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
                loss.backward()
                
                if (step + 1) % self.gradient_accumulation_steps == 0:
                    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                    
                    # ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
                    if self.scheduler:
                        self.scheduler.step()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        step_time = time.time() - start_time
        throughput = batch["input_ids"].size(0) / step_time
        
        return {
            "loss": loss.item() * self.gradient_accumulation_steps,
            "step_time": step_time,
            "throughput": throughput,
            "learning_rate": self.optimizer.param_groups[0]["lr"] if self.optimizer else 0
        }
    
    def generate_optimized(self, input_text: str, max_length: int = 200, 
                          temperature: float = 0.7, num_beams: int = 4) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø­Ø³Ù† Ù„Ù„Ù†Øµ"""
        start_time = time.time()
        
        with self.memory_monitoring():
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            inputs = self.model.tokenizer(
                input_text, 
                return_tensors="pt", 
                max_length=256, 
                truncation=True
            ).to(self.device)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª
            with torch.no_grad():
                if self.mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = self.model.generate(
                            **inputs,
                            max_length=max_length,
                            temperature=temperature,
                            num_beams=num_beams,
                            no_repeat_ngram_size=2,
                            do_sample=True,
                            top_p=0.9,
                            top_k=50,
                            early_stopping=True,
                            pad_token_id=self.model.tokenizer.pad_token_id,
                            eos_token_id=self.model.tokenizer.eos_token_id,
                            use_cache=True  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
                        )
                else:
                    outputs = self.model.generate(
                        **inputs,
                        max_length=max_length,
                        temperature=temperature,
                        num_beams=num_beams,
                        no_repeat_ngram_size=2,
                        do_sample=True,
                        top_p=0.9,
                        top_k=50,
                        early_stopping=True,
                        pad_token_id=self.model.tokenizer.pad_token_id,
                        eos_token_id=self.model.tokenizer.eos_token_id,
                        use_cache=True
                    )
            
            # ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ù†ØªÙŠØ¬Ø©
            generated_text = self.model.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        generation_time = time.time() - start_time
        logger.info(f"âš¡ ÙˆÙ‚Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {generation_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        
        return generated_text
    
    def batch_generate_optimized(self, input_texts: List[str], 
                                batch_size: int = 4) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø§Øª Ù…Ø­Ø³Ù†"""
        results = []
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
        for i in range(0, len(input_texts), batch_size):
            batch_texts = input_texts[i:i + batch_size]
            
            with self.memory_monitoring():
                # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¯ÙØ¹Ø©
                inputs = self.model.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    max_length=256,
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ
                with torch.no_grad():
                    if self.mixed_precision:
                        with torch.cuda.amp.autocast():
                            outputs = self.model.generate(
                                **inputs,
                                max_length=200,
                                temperature=0.7,
                                num_beams=4,
                                no_repeat_ngram_size=2,
                                do_sample=True,
                                early_stopping=True,
                                pad_token_id=self.model.tokenizer.pad_token_id,
                                eos_token_id=self.model.tokenizer.eos_token_id,
                                use_cache=True
                            )
                    else:
                        outputs = self.model.generate(
                            **inputs,
                            max_length=200,
                            temperature=0.7,
                            num_beams=4,
                            no_repeat_ngram_size=2,
                            do_sample=True,
                            early_stopping=True,
                            pad_token_id=self.model.tokenizer.pad_token_id,
                            eos_token_id=self.model.tokenizer.eos_token_id,
                            use_cache=True
                        )
                
                # ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                batch_results = self.model.tokenizer.batch_decode(outputs, skip_special_tokens=True)
                results.extend(batch_results)
        
        return results
    
    def profile_model(self, input_text: str, num_runs: int = 10) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        logger.info(f"ðŸ” ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ {num_runs} ØªØ´ØºÙŠÙ„...")
        
        times = []
        memory_usage = []
        
        for i in range(num_runs):
            start_time = time.time()
            
            if self.device.type == "cuda":
                torch.cuda.synchronize()
                start_memory = torch.cuda.memory_allocated()
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            _ = self.generate_optimized(input_text)
            
            if self.device.type == "cuda":
                torch.cuda.synchronize()
                end_memory = torch.cuda.memory_allocated()
                memory_usage.append((end_memory - start_memory) / 1024**3)  # GB
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        avg_time = np.mean(times)
        std_time = np.std(times)
        avg_memory = np.mean(memory_usage) if memory_usage else 0
        
        profile_results = {
            "average_time": avg_time,
            "std_time": std_time,
            "min_time": np.min(times),
            "max_time": np.max(times),
            "average_memory_gb": avg_memory,
            "throughput_per_second": 1.0 / avg_time,
            "device": str(self.device),
            "mixed_precision": self.mixed_precision
        }
        
        logger.info(f"ðŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        logger.info(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª: {avg_time:.3f} Â± {std_time:.3f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {avg_memory:.2f} GB")
        logger.info(f"   Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©: {1.0/avg_time:.2f} Ù†Øµ/Ø«Ø§Ù†ÙŠØ©")
        
        return profile_results
    
    def cleanup_memory(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
        logger.info("ðŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
    
    def get_performance_summary(self) -> Dict:
        """Ù…Ù„Ø®Øµ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        summary = {
            "device": str(self.device),
            "mixed_precision": self.mixed_precision,
            "gradient_accumulation_steps": self.gradient_accumulation_steps,
            "optimizer": type(self.optimizer).__name__ if self.optimizer else None,
            "scheduler": type(self.scheduler).__name__ if self.scheduler else None,
            "performance_stats": self.performance_stats
        }
        
        return summary

class OptimizedDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø³Ù†Ø©"""
    
    def __init__(self, data: List[Dict], tokenizer, max_length: int = 256):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # Tokenization Ù…Ø­Ø³Ù†
        inputs = self.tokenizer(
            item["input"],
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        
        labels = self.tokenizer(
            item["output"],
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        
        return {
            "input_ids": inputs["input_ids"].squeeze(),
            "attention_mask": inputs["attention_mask"].squeeze(),
            "labels": labels["input_ids"].squeeze()
        }

def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    print("ðŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ‡Ù…ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
    
    tokenizer = AutoTokenizer.from_pretrained("t5-small")
    model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡
    optimizer = PerformanceOptimizer(model)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
    optimizer.setup_mixed_precision(enabled=True)
    optimizer.setup_gradient_accumulation(steps=4)
    optimizer.setup_optimizer()
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
    optimizer.optimize_model_for_inference()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    test_input = "Create a healthy meal plan for a 30-year-old woman"
    result = optimizer.generate_optimized(test_input)
    
    print(f"âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {result}")
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
    profile = optimizer.profile_model(test_input)
    print(f"ðŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡: {profile}")

if __name__ == "__main__":
    main()
